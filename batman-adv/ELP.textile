= Neighbourhood Discovery Protocol =
B.A.T.M.A.N., no matter if batmand or batman-adv are not relying on a specific set of drivers to determine the link qualities towards neighbors. Instead, it counts the neighbors' originator messages and the own originator messages rebroadcasted its neighbors to calculate the tx-quality. So basically, OGMs are doing two jobs, calculating the link qualities and spreading topology information through the whole mesh.

However, this also has three major drawbacks: 
 * You usually should set the originator interval to a value suiting the most mobile node on all nodes, having a homogeneous interval. This means, that even if you have a very static neighbourhood with lots of nodes (so resulting to a lot of rebroadcasts, which can be the case on VPN- or ethernet interfaces used by batman-adv), you can't lower the interval for that one because very mobile nodes far away might than not be able to reach you anymore.
 * A OGM should not be rebroadcasted by the same node more than once because that could falsify the link-quality calculations. However you might want to do this to speed the convergence at more distant and/or weaker links.
 * It might be generally desireable to use quite fast probing intervals for link-quality calculations, as the local link can change quite quickly, but not propagating at the same speed yet because it might still be changing. For instance, a link could go down from 100%25 to 50%25. batman would go down and propagate this step by step, linearly within ~30 seconds (at default intervals), For instance, detecting the break-down from 100%25 to 50%25 might better be noticed in less than a second, and propagating changes still every second. However such an interval-split is not possible with OGMs having two purposes right now.

The idea is to strip the link-quality calculations from the normal originator-messages and use a seperate packet type (basically hello-messages) and mechanism for this. Which would allow us to develop (convergence) optimization strategies for both jobs independently.

== Pros of NDP ==
 * Modularization of the code 
 * The delta changes between two OGMs can be increased with an NDP interval faster than the OGM interval, resulting in a higher influence of a single OGM. 
 * Seperate optimization strategies can be used for NDP and OGMs individually then. 
 * The more sparse the network is (number of single hop neighbors significantly smaller than the number of all nodes), the faster the NDP interval can be chosen relative to the OGM interval, resulting in major convergence speed improvements in sparse networks. 
  * As NDP messages are never rebroadcasted it also helps to reduce the overhead in dense networks (many single hop neighbors).  
   * (Mobile nodes can chose a faster OGM + NDP interval then. It directly improves their transmit path - which was not the case with the OGM LQ measurements due to the echo-quality dependency, and indirectly improves their receive path a little, due to the asymmetric penalty) 
   * Less overhead: no rebroadcasts
   * More acuracy in case of medium/bad links

== Definitions ==

 * duplicate
 * out of order
 * LQ / RQ -> in percent
 * bidrectional link

== Protocol Procedure ==
every node broadcasts a NDP packet which contains a seqno and the list of neighbors it can hear.
[Picture of NDP packet] (not struct definition)

=== Calculating Receive Quality ===
 * Counting NDP packets
 -> duplicate, abort proceding

=== Calculating Transmit Quality ===
 * don't attach neighbor if RQ == 0
 * 
 
== Misc ==

=== Route switching ===
NDP does not directly change routing decisions on its own, to keep a clean separation between OGMs and NDP and to reduce side effects.

=== Interface down ===
 * Clear LQ table

== Extensions ==
=== Threshholds ===
=== EWMA ===
=== Weighted Window ===
=== Auto RQ window sliding ===
=== Fixed minimum NDP packet size? ===
=== Sometimes maximum packet size packets? ===



----

== RQ calculation ==
In periodic intervals, customizable per interface, a ndp packet gets broadcasted on each interface. As it has been done with the normal OGMs before, the rq-value is being calculated with a sliding window and counting the number of received ndp packets, but on a per interface basis now.

== local TQ propagation ==
Additionally, a 2-tupel of neighbor-adresses and rq-values of neighbors on a certain interface will be attached to each ndp packet of the according interface. A neighbor will then pick the rq-value which the other node determined from the received ndp packets and insert this value as its own tq-value towards that node.

== global TQ propagation ==
A node receiving an OGM will reduce its global TQ field by the local TQ fitting the neighbor and interface before processing/rebroadcasting (but after validation/duplicate checks).

== Data packet forwarding ==
This will still be done according to the received global TQ.

----

== Further optimizations ==

=== Consider local-TQ for a final forwarding ===
It might happen, that a node rapidly moves towards the sender of a data packet. If so, a forwarding node can notice, that the local TQ value towards the final destination of the data packet is suddenly better than the global TQ via another hop. This forwarding node can then forward the packet to its final destination instead of forwarding it to the other next-hop according to the global TQ.

=== Rebroadcast OGM X times instead of once ===
Every node could rebroadcast an OGM i.e. 3 times instead of just once to increase the probability of an OGM reaching every node in the mesh, resulting in a much better convergence speed (getting closer to linear convergence speed) for the trade-off of control overhead (getting closer to squared).

=== Unscheduled OGM rebroadcasts ===


=== Adaptive ndp interval ===

=== Probablistic flooding ===
Although this does not optimize the convergence speed, it could greatly increase the efficiency of the flooding of broadcast/mulitcast-data packets. With ndp we now also know our neighbors' TQ-values on the same interface. With this information a node A can determine the probability, that a broadcast of node B has already reached all of A's neighbors:

P(|N| = |N^+^|) = ∏_{n ∈ N} TQ(B, iface, n) with N being the set of A's neighbors and N^+^ the set of neighbors successfully receiving the broadcast.

A node could then either decide with a threshold, if it might not rebroadcast (rebroadcast, if P(|N| = |N^+^|) < 0.9 i.e.). Or more elegantly rebroadcast to a certain probability: 1 - P(|N| = |N^+^|) (or 1 - P(|N| = |N^+^|)^3^ for a more conservative decision i.e.).

----


==== Looping problems ====

''Not an NDP issue, also present for current OGM mechanism - moved it to a different page later''

However, there seems to be a looping problem? (or do sequence numbers or global TQ window fix this?)
In the following 4 node scenario, the parameters are as follows:
 * OGM interval: 1000
 * NDP interval: ε (very fast)
Now node R moves from B to A within the first second, so he'll arrive at B before it schedules its next OGM. Also this next OGM after this first second already uses the new link qualities due to the fast NDP interval.

'''t = 0:'''

[[Image(NDP-mobile-loop-0s.png, 20%25)]]

'''t = 1:'''

[[Image(NDP-mobile-loop-1s.png, 20%25)]]

After about 10 seconds average, there will be a loop with S sending packets to B, and B sending them back to S. This loop will last for about 3.3 seconds average.

The following [[http://x-realis.dyndns.org/freifunk/scan.pdf table]] will hold some more in detail information with the times and events that will change BATMAN nodes topology knowledge. It holds the following information:
 * The times t in the table are the average times of arrival of an OGM at a node in seconds.
 * The table shows the TQ values towards R. The first six of these rows are from a nodes perspective, the last two ones the actual, real TQ values for the chosen path.
 * Time span with loops is marked with YES in the 'loops' row.

''Result:'' In this quite typical and small-scale scenario, we already have an average time with looping packets of '''3.3''' seconds.
